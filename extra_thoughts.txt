######
ASIDES
######

##################
Network Throttling
##################

     Something that Dockerfile and cgroups don't do is throttle network
traffic for a service.  The most upvoted question in the Kubernetes Dev Team
AMA on reddit (4/10/18) is a question about adding network limits to cgroup
settings, and the answers from the devs were quite illuminating on the subject.
Basically they say it's hard to limit bandwidth because you'd need a controller
on each end, sender and receiver, since network bandwidth isn't a property of a
pod - it's a property of the path from one endpoint to another. Their answers
on that question and other questions led me to research Envoy, which is a
really interesting program.

      From what I've gathered thus far, Envoy gets installed on every pod and
nall traffic to and from that pod is proxied through Envoy.  Thus each instance
of Envoy would be able to talk to the receiving service's Envoy instance, and
determine how fast the data should be sent from one side to another.  Honestly
I'm probably wrong in some of that but it sounds... right, I suppose.  I'm
gonna do a lot more research on Envoy.  I'm really curious how they decide to
drop packets if the rate is limited and the queue for packets gets to be too
long, because I could see a situation where there's a long queue when limits
are enforced yet network traffic is low.  That's probably an easy fix in the
protocol though.  All the stuff I've heard about Envoy so far reminds me of
MPLS and some of the other L2 protocols I learned about back when I was
studying for the CCNA.  I'm curious if there's anything at the lower level that
could be adapted for this issue. 


########
Security 
########

    The devs put this app in production - so anyone on the web can access it,
right?  And the log file is being stored on the SAME MACHINE as the app, right?
So, if an attacker were able to compromise the app, even if the attacker
couldn't get root the log file could be modified (to varying degrees, depding
on if the app had read/write privileges or just append only). Worst case
scenario, would be if the attacker were to gain root on the machine, nothing in
the log file would be trustworthy.  The situation makes me think of a race
condition: until the log file is secure, it can't be determined if any
information in it is accurate.

    Thus my first step would be to cross my fingers & pray that the app hasn't been
compromised yet.  If I could, I'd write a script to install & configure rsyslog
on every deployed server to get those log files stored remotely ASAP.  If
rsyslog wasn't an option, I'd write a small app that would monitor the log file
for changes and write those changes to a remote destination, making sure that
the receiving app at the destination could only append log files.  There's
still a vulnerability that the receiving log app would be vulnerable to
malformed RPC from a compromised main app, but it's still a lot more secure of
an implementation.

    Remote storage is beneficial in other ways, beyond security. If the main app
crashes or the host is DDOS'ed, you'd want the log files stored remotely so
that the devs could see what led up to the outage.

HOWEVER, this stuff is obviously beyond the scope of the assignment, but it was
really bugging me so I felt like I should write it down.


###################
Configuration Aside
###################

   Ooooh boy.  So there's a feeling I've been getting a lot lately - that feeling
when you uncover a huge topic in the tech world that you never knew about and
you're flabbergasted that you missed it until now.  It's 4/10/2018 and I just
got that feeling for configurations in deploying cloud native applications.

The first configuration steps I took for this problem were to use Go's "flag"
package.  It's so much easier compared to reading argv[] in C that I probably
overdid it.  I had a list of like 5-10 flags that the CLI passed when it ran
the logserver, so I decided to look up config files in Go.  Folks on
StackOverflow reccomended using JSON in a config.json file, so I went with
that.  Seems a lot easier than having to keep track of all the flag settings
passed in at exec time - especially when it's a deployed app and you want to
automate any restarts.

AND THEN I watched a lynda.com video last night from a guy with 20 years
experience and is a self-professed "total Cloud Native nerd" - Leander Reimer.
In his Go code, he pulls configurable variables from the environment!  It kinda
blew my mind, cause it's such a pain in the butt in C that I never really
considered it.

[paraphrasing from Leander Reimer] - "implementing a helper function to extract
the port from an environment variable is an example of good practice in cloud
native application development - you want things configurable via environment
variables"

So that changes things a bit.  Once I saw how you could use Dockerfiles to set
environment variables in the parent process, it really made a lot of sense to
me.  I do have some concerns still, however - if that parent process is
starting multiple children, wouldn't the names of environment variables have to
be unique to not overwrite each other?  You'd have to enforce a naming policy -
easy in theory, hard in practice, like getting people to use hard passwords.

I did some more digging, and the top response to a discussion on StackOverflow
was very illuminating.

https://stackoverflow.com/questions/7443366/argument-passing-strategy-environment-variables-vs-command-line?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa

Definitely read the response from Matt Fenwick.  It's 7 years old, and before
the cloud app takeover, but I bet a lot of it holds true.  Overall he argues
heavily against env vars, and gives examples of the name conflict bugs i was
worried about. He's a proponent of using all 3 methods - flags, env vars, and
config files - so I guess I'll just have to learn the situations in which to
use the proper tool for the job. Interestingly, he distinguishes between real
config files and the config.json file that I've been using.  He calls it the
`command-line parameter-file` approach, because it doesn't store user-set
configuration data.  I suppose a good method for my logserver would be to check
passed-in arguments against a saved config file, and update the config file if
the values differ, then set program variables from the config file. Really
curious what you Apptio guys do - probably something akin to the right tool for
the right job.

https://www.mirantis.com/blog/how-do-you-build-12-factor-apps-using-kubernetes/

This blog post is a great read for the config debate.  It was written 4/17/2017
and focuses specifically on cloud native apps with kubernetes. Looks like the
debate still rages on.


#################
Further questions
#################

After a week or so of working on this Readme, the questions are starting to
pile up.  I'm gonna try to keep a running set of questions I still need
answered down here.

- is there a sigsetjmp/siglongjmp in Go?  Is such a thing allowed for a
  containerized process in the event of a segfault?

- what kinds of security measures are used to ensure only admins connect with kubectl,
  beyond ACL's and PKI stuff?  Can kerberos be run somehow, or is that too over the top?

- environment variables vs config files vs command line flags for cloud native apps?

- does Apptio have any code or ideas they'd want to merge into the mainline Linux kernel?

- is sharing a log file between the docker host and a running container common practice?

- what's the current method of limiting network bandwidth of a service, if any?


##############################
DESIGN/IMPLEMENTATION THOUGHTS
##############################


    My plan: install an HTTP server on the main app host machine, listening on
a port that is inaccessible from outside the firewall.  Pretty simple,
just add a rule in iptables/ACL preventing WAN access on that port, and test it
later from the outside with noisy nmap scans.  Thus the app would only respond
to GET requests originating from inside the organization, and could reply with
JSON, plain text, or HTML.

   However, that HTTP design completely depends on the devs accessing the
server from a private IP address.  If the main app were in a container out in
the cloud, this would be a little trickier.  Port forwarding/NAT could be done
for requests coming from a specific range of IP addresses, but an attacker can
easily spoof his/her IP.  Another option would be to write a client side proxy log
server that uses an SSH tunnel to connect with the log server on the main
machine.  A client side proxy would solve many other issues as well, by sort of
trunking requests from devs so that the main machine log server would only need
to track one TCP connection.

  From what I've seen, kubectl has the same issue - insider access for devs,
but none for the clientele.  Thus, if the main app in the problem is
cloud-based, I'd have to either
  a) install keys for each dev
  or
  b) write a client-side proxy server that operates in the organization's
     internal network, and install a key for that server in the worker nodes
     for the main app.
  or
  c) have some kind of ticket authentication/authorization service like kerberos
     configured for the log server.

    Writing a client-side proxy is definitely a goal of mine, if I get that
far. Log entries will never be changed - so if the client requests a search of
entries in a range that the proxy has cached, there's no need to ask the main
app server for a log update.  In the client-side program, caching would reduce
network load.  In the server side program, caching would reduce system calls &
disk I/O requests, leaving more resources available for the main app.

Further ideas/issues:
     - have the server take a conf file so the admin doesn't need a long string of flags
        for each startup of the program
	   *** did this, but see the configuration section for more details.  it's
	       crazy how dependent the app is on deployment - basically, need to come up with
	       deployment before you start coding configurations.
	       
     - big read requests of the log file could slow down the main app
         Possible solutions:
           - add a limit to how much a user can access at one time
	   - only run the search when the CPU isn't under heavy load
	   - before any requests, cache as much of the log file into memory as possible
	   - cache results (both client side and server side)
	   - depending on the OS, change the 'nice' level of the log server process
         	   *** turns out all this cgroup stuff can easily be implemented by Dockerfile
	
     - the server side app will need read access on the log file and any
        archived log files.
     - network traffic of sending log search results could impact performance for main app
     - upgrades to the server side program API shouldn't break clients that assume
         an older API
     - if using a proxy, could batch requests & send as one to the log server
     - filtering the log file kinda reminds me of MapReduce; curious if there's anything I can
        do with that. 
     - instead of passing a conf file location to main in argv[], could write a few CLI prompts
       that, when the server is first executed, ask the user for either a config file location
       AND/OR values to use instead.
     - add a way to restart the logserver if it gets a sigsegfault or sigkill or any other
       signal that terminates the process.  Back in C you could use setjmp and sigsetjmp
       to save the process's environment early on in main, and jump back to that spot with
       a sig handler function that called siglongjmp.  I'm curious if there's a way to do
       that in Go, or if such a solution is even allowed in a container.  Definitely a
       question I'll be bringing up during my on-site interview. 
       

BIG ISSUE:
      As of 4/13/2018, every request to the logserver reads the entire log
   file in from disk (or the kernel's file cache buffer if it hasn't been
   evicted - no control over that though).  This will not scale if the log file
   to read gets to be terrabytes in size.  There's a few solutions that differ
   depending on if the file is small enough to fit in memory or not.
      - if the log file can comfortably fit in memory, then we should load & parse
        the entire file at runtime & save it as a global array of log entries.
	We could also save the byte offset where we last encountered an EOF when
	parsing the log file (or the size of the file itself).  Then whenever a request
	comes in to read the log file, we'd only need to check if the log file
	is bigger now than it was during the last parse, and if it was, lock the
	global array and update all the globals.
	With this approach, a search by date of log entries wouldn't need to check
	for updates if the range of dates being searched is contained within the
	array of logentries.  However, if searching for a word in a log message, 
	the log file would still need to be checked for updates.
	 - This introduces a few more problems, however.
	   1. If an attacker can't change the values in the log file itself,
              he/she could change the values in the global array of log entries
	      in the logserver to cover his/her tracks.
	   2. a lock on the global array of logentries creates a new host of
	      issues.  If there are a ton of updates since the last read to
	      parse and the CPU resources given to the logserver cgroup is low,
	      devs accessing the logserver could spend a while waiting for a
	      response.  
      - If the log file CANNOT fit in memory, every client request would have to
        read in chunks at a time of the log file, parse it, send it, and repeat.
	This would eliminate the possibility of concurrent requests, since a single
	client would be using all the memory available to the cgroup.  In this situation,
	having a client-side proxy server would be almost mandatory. The proxy would
	have more RAM and CPU power available, giving it a better caching power and thus
	a better ability to serve concurrent requests.  

BIG ISSUE:
       I don't know how the <datetime logtime> is formatted.  Go provides a
   time.Parse function that turns strings into Time values, but it needs either
   an example string or one of the constants in the time library.  I thought
   about having a control flow switch if no example string is provided, meaning
   it would keep the datetime entry as a string, but since the other case would
   return a value of Time, I'd have to have two different structs for log
   entries.  Go doesn't have unions/one-of types, so I could do something clever
   with interfaces here to have the code work in both cases.
       My current solution splits the log entries up by "," and assumes that the
   first comma is the first byte after the datetime string.  If the datetime format
   used in the main app log file has commas in it, this will break the output
   format and any searchability.
       In the real world, this problem wouldn't be an issue because the format
   would be known ahead of runtime, and the logserver could refuse to run without
   a proper datetime format specified for parsing the log file.  I can't allow a
   default example string format, because it's an assumption that could cause a lot
   problems if it were accidentally used. 



###############
Go compiler bug
###############

// This code has a runtime error in the final line, as err is undefined,
// but triggers a compile time error if := is used because 'no new definitions on left side'.
// since err was assigned up top & the compiler can't tell the difference.
// easy fix: rename the second err

var logfile *os.File

func main() {


	if len(os.Args) > 1 {
		confFile, err := os.Open(os.Args[1])
		if err == nil && !os.IsNotExist(err) {
			defer confFile.Close()

			err = json.NewDecoder(confFile).Decode(&conf)
			fmt.Println(conf)
			if err != nil {
				log.Fatal(err)
			}
			
		} else if err != nil {
			log.Fatal(err)
		}
	}

	logfile, err = os.Open(conf.Dir + conf.Logfile)



