
Problem A 

#######################
DESIGN & IMPLEMENTATION
#######################

    My plan: install an HTTP server on the main app host machine, listening on
a port that is inaccessible from outside the firewall.  Pretty simple,
just add a rule in iptables/ACL preventing WAN access on that port, and test it
later from the outside with noisy nmap scans.  Thus the app would only respond
to GET requests originating from inside the organization, and could reply with
JSON, plain text, or HTML.

   However, that HTTP design completely depends on the devs accessing the
server from a private IP address.  If the main app were in a container out in
the cloud, this would be a little trickier.  Port forwarding/NAT could be done
for requests coming from a specific range of IP addresses, but that stuff can
be spoofed easily.  Another option would be to write a client side proxy log
server that uses an SSH tunnel to connect with the log server on the main
machine.  A client side proxy would solve many other issues as well, by sort of
trunking requests from devs so that the main machine log server would only need
to track one TCP connection.

  From what I've seen, kubectl has the same issue - insider access for devs,
but none for the clientele.  Thus, if the main app in the problem is
cloud-based, I'd have to either
  a) install keys for each dev
  or
  b) write a client-side proxy server that operates in the organization's
     internal network, and install a key for that server in the worker nodes
     for the main app.
  or
  c) have some kind of ticket authentication/authorization service like kerberos
     configured for the log server.

    Writing a client-side proxy is definitely a goal of mine, if I get that
far. Log entries will never be changed - so if the client requests a search of
entries in a range that the proxy has cached, there's no need to ask the main
app server for a log update.  In the client-side program, caching would reduce
network load.  In the server side program, caching would reduce system calls &
disk I/O requests, leaving more resources available for the main app.

Further ideas/issues:
     - have the server take a conf file so the admin doesn't need a long string of flags
        for each startup of the program
	   *** did this, but see the configuration section for more details.  it's
	       crazy how dependent the app is on deployment - basically, need to come up with
	       deployment before you start coding configurations.
	       
     - big read requests of the log file could slow down the main app
         Possible solutions:
           - add a limit to how much a user can access at one time
	   - only run the search when the CPU isn't under heavy load
	   - before any requests, cache as much of the log file into memory as possible
	   - cache results (both client side and server side)
	   - depending on the OS, change the 'nice' level of the log server process
         	   *** turns out all this cgroup stuff can easily be implemented by Dockerfile
	
     - the server side app will need read access on the log file and any
        archived log files.
     - network traffic of sending log search results could impact performance for main app
     - upgrades to the server side program API shouldn't break clients that assume
         an older API
     - if using a proxy, could batch requests & send as one to the log server
     - filtering the log file kinda reminds me of MapReduce; curious if there's anything I can
        do with that. 
     - instead of passing a conf file location to main in argv[], could write a few CLI prompts
       that, when the server is first executed, ask the user for either a config file location
       AND/OR values to use instead.
     - add a way to restart the logserver if it gets a sigsegfault or sigkill or any other
       signal that terminates the process.  Back in C you could use setjmp and sigsetjmp
       to save the process's environment early on in main, and jump back to that spot with
       a sig handler function that called siglongjmp.  I'm curious if there's a way to do
       that in Go, or if such a solution is even allowed in a container.  Definitely a
       question I'll be bringing up during my on-site interview. 
       

BIG ISSUE:
      As of 4/13/2018, every request to the logserver reads the entire log
   file in from disk (or the kernel's file cache buffer if it hasn't been
   evicted - no control over that though).  This will not scale if the log file
   to read gets to be terrabytes in size.  There's a few solutions that differ
   depending on if the file is small enough to fit in memory or not.
      - if the log file can comfortably fit in memory, then we should load & parse
        the entire file at runtime & save it as a global array of log entries.
	We could also save the byte offset where we last encountered an EOF when
	parsing the log file (or the size of the file itself).  Then whenever a request
	comes in to read the log file, we'd only need to check if the log file
	is bigger now than it was during the last parse, and if it was, lock the
	global array and update all the globals.
	With this approach, a search by date of log entries wouldn't need to check
	for updates if the range of dates being searched is contained within the
	array of logentries.  However, if searching for a word in a log message, 
	the log file would still need to be checked for updates.
	 - This introduces a few more problems, however.
	   1. If an attacker can't change the values in the log file itself,
              he/she could change the values in the global array of log entries
	      in the logserver to cover his/her tracks.
	   2. a lock on the global array of logentries creates a new host of
	      issues.  If there are a ton of updates since the last read to
	      parse and the CPU resources given to the logserver cgroup is low,
	      devs accessing the logserver could spend a while waiting for a
	      response.  
      - If the log file CANNOT fit in memory, every client request would have to
        read in chunks at a time of the log file, parse it, send it, and repeat.
	This would eliminate the possibility of concurrent requests, since a single
	client would be using all the memory available to the cgroup.  In this situation,
	having a client-side proxy server would be almost mandatory. The proxy would
	have more RAM and CPU power available, giving it a better caching power and thus
	a better ability to serve concurrent requests.  


BIG ISSUE:
       I don't know how the <datetime logtime> is formatted.  Go provides a
   time.Parse function that turns strings into Time values, but it needs either
   an example string or one of the constants in the time library.  I thought
   about having a control flow switch if no example string is provided, meaning
   it would keep the datetime entry as a string, but since the other case would
   return a value of Time, I'd have to have two different structs for log
   entries.  Go doesn't have unions/one-of types, so I could do something clever
   with interfaces here to have the code work in both cases.
       My current solution splits the log entries up by "," and assumes that the
   first comma is the first byte after the datetime string.  If the datetime format
   used in the main app log file has commas in it, this will break the output
   format and any searchability.
       In the real world, this problem wouldn't be an issue because the format
   would be known ahead of runtime, and the logserver could refuse to run without
   a proper datetime format specified for parsing the log file.  I can't allow a
   default example string format, because it's an assumption that could cause a lot
   problems if it were accidentally used. 

############
ALTERNATIVES
############

 - log.Fatal(http.ListenAndServe(":8080", http.FileServer(http.Dir("/usr/share/doc"))))
     If the log file were used in that http.Dir call, the logserver program would be
     ~10 lines of code.
	-problem: pretty brittle.  If the log file gets large or archived, updates to 
	the logserver would be needed to accomodate changes.
        However, lazyness is a virtue.  It's the simplest & quickest solution
        to the problem we're trying to solve.
	       
- Give every dev root access on the app server
    - Not an option, because of how easy that privilege can be abused.

- Write a caching client proxy server that forwards dev requests.  If all devs go through
   the proxy, there will be a large reduction in CPU, RAM, disk I/O, and network resources
   used by the log server on the main app host machine. 
   - problem:
	It's a little beyond the scope of the problem we're trying to solve, and introduces
	a security vulnerability. Without more information, it's impossible to know if caching
	log files external to the server is allowed. Also, a malicious actor could cover his
	tracks by changing values saved in the cache.
      
- Add rsyslog to the main app host machine & copy the log files to an external database
     -problem: 
  	Not enough information is known about the situation.  Rsyslog is more resource
	intensive than my small log server implementation - maybe smaller is better. 

- Add Kerberos to the network & check tickets whenever log files are requested.
   Adding authentication/authorization to the log server would allow for a much
   more dynamic API.  Server admin specific commands could be added, e.g. change configs
   without having to restart the server.
      -problems:
        - increases complexity of the system as a whole
        - Even though main app customers wouldn't need to go through kerb, they might still
	  see slowdowns.  A dev might write a script that grabs log files every 10 seconds,
	  and overhead of decrypting/encrypting those messages by the server app on the host machine
	  could bog down the system.
	- This is beyond the scope of the main problem - getting the devs access to the log files.
    
- Chmod the log files to a group on the host OS, give that group read privileges,
   give devs an account on the server they can SSH into, and add that account to the group. a cron job
   could be added to the host to add log archive files to that group with read privileges.
     -problems:
         - assumes the security policy is ok with devs having ssh access to the machine.
	 - would be hard to track which dev is using the account and at what time

- Add in a sandbox program to let devs pass in their own functions to filter the log
     -problem:
	complexity.  Something like this would take a long time to build, test, and deploy.
        

###############
DEPLOYMENT PLAN
###############

Deploying such a server would depend heavily on the infrastructure design.  In
the most basic scenario, an admin could write a script that run like so:

a) scp the log server binary that is arch/OS specific to the host machine into
   the same namespace as the log files
    - and don't forget to scp the conf.json file as well
b) ssh to the host machine
c) chmod/chown the binary
    - make sure the server binary isn't root
    - create a group that only the binary & the log files are part of
    - give the group read only privileges to the log files
d) write either a cron job or a systemd service that adds any new or archived
   log files to the group
e) have cron or systemd start/restart the server when needed
f) chroot the logserver binary, keeping it in the same namespace as the log file
g) add the logserver to a cgroup & limit it's CPU/RAM resource availability
f) On the gateway machine/router connected to the WAN, modify NAT/port
    forwarding to prevent external logserver access, and/or add a rule to the ACL
    preventing access from WAN IP's to the port used by the server.
    


###########################
MONITORING/REPORTING/ALERTS
###########################

Things to log:
  a) the config file used at runtime
       - the entire file itself, or just a few parameters if too long
  b) request IP origins for each interaction, possibly what they request as well
  c) any errors, of course
  d) could log user agents of the connecting devices, to focus future development
     on a specific hardware/software.  Random, but got the idea from a CMU lecture.
     
Alerts:
  - Send an alert if one IP address is making a ton of requests -
     having the main app be DDOSed by a log server wouldn't be fun to explain to
     clients of the main app.

Monitoring:
  - use heartbeats from an external source to make sure the server is still running.
      In cloud architectures, I think that's what "circuit-breakers" are for, but
      as of 4/12/18 I still don't have a full view. 
      In an old-school deployment, I could set up a cron job that uses netcat or
      some other net suite program to send a HEAD request to the log server.
      If the server didn't respond for a few requests, sound the alarms!  Could
      send a text to a sysadmin.

Interestingly, Go makes it easy to have a log file on a different machine -
output could be written to a socket, since log.SetOutput takes an io.Writer
interface as the parameter.  Easy remote logging!  Especially nice if the
output logging file descriptor is duped, causing writes to both a local file
and to the socket file, in order to make it fault tolerant.


#############
CONFIGURATION
#############

There's lots of parameters to set on the logserver.  One area of parameters
would be cgroup settings like maximum memory allowed, max cpu resources, stuff
like that, which would be set in the Dockerfile.

There's also monitoring alert settings - where to send alerts, where and how often
to send out heartbeat messages, and trigger levels for auto banning an IP to prevent
DOS attacks.

Then there's program specific configuration, such as
 a) port to bind on
 b) maximum cache size
 c) location of log file for the log server
 d) config file location
 e) could enable or disable specific RPC methods
 f) where the location of the main app's log file is
 g) what the time ordering is in <datetime logtime> - dd/mm/yy, mm/dd/yy, etc.
    - did some more digging: the time pkg in go has a parser for strings,
      but it needs either the RFC corresponding to the layout, or an example
      of the form to be used by the parser in time.Parse()

      Since there's no authentication/authorization of the incoming requests, every
configuration value must be set on execution.  If anyone could change log
server settings via RPC without a/a, the settings would be pretty meaningless.


#######
TESTING
#######

For unit testing, I got a ton of help from "The Go Programming Language".
 From page 316 -
  "A good test does not explode on failure but prints a clear and succint 
   description of the symptom of the problem, and perhaps other relevant facts
   about the context."
 From 317 -
  "... a test that spuriously fails when a sound change was made to the program
   is called brittle."
 "TGPL" helped me write table-driven testing, and explained why "log.Fatal(err)"
   calls should only be the decision of main(), and tons of other ideas.  Amazing book!

Beyond unit testing:

 -  When deployed, use noisy nmap scans from outside the organization to make
      sure the log server port is inaccessible from the WAN.
 - Could use a fuzzer on the REST api, but I think that would just expose broken
      parsing in the Go libraries.  Still probably a good thing to do.

   Most important of all is testing to ensure that the log server doesn't slow
down the main app when installed on the same host.  We need to run a copy of
the main app alongside the log server on a host machine not in production, and
see what happens when both the main app and the log server are operating under
heavy request volume.  If the logserver ends up hogging the CPU, limiting CPU
availability for the log server by modifying it's cgroup would be easy.
However, I've been reading that limiting network bandwidth of a
cgroup/container is a tricky issue that hasn't yet been solved.


#########
RESOURCES
#########

For this assignment, I mostly stuck to examples in the Go pkg documentation at
golang.org/pkg/ and examples in "The Go Programming Language".
Staying away from code frameworks like Gorrilla Mux minimized my tech debt. 

 - "The Go Programming Language" 

 -  golang.org/pkg/*

 -  MIT 6.824 Distributed Systems Lecture 7 -
     https://www.youtube.com/watch?v=grrJxx9PKqE

 -  unix man pages

 -  MIT 6.858 Security - Lecture 4: Privelege Seperation in Web Apps
     https://www.youtube.com/watch?v=XnBJc3-N2BU

 -  https://stackoverflow.com/questions/16465705/how-to-handle-configuration-in-go?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
     "How to handle configuration in Go"

 - https://www.reddit.com/r/kubernetes/comments/8b7f0x/we_are_kubernetes_developers_ask_us_anything/
    
 - lynda.com video series "Getting started with Cloud Native Go" & "Advanced Cloud Native Go"
     for reading env vars:
       fun port() string { 
        port := os.Getenv("PORT") 
        return ":" + port 
       }

 - CMU's 15-418 Parallel Computer Architecture & Programming 2016 - Lecture 14 - Scaling a Website
    https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=3bb2f332-fbdb-4434-9f3b-0c2b3f9668c8


   


   




