########
PROLOGUE
########

    This README file got pretty long, so I decided to write a little background
on WHY it's so long.  My girlfriend is a horse trainer, and I like to tag along
and spectate.  There's lots of downtime at horse shows, hence lots of time to
brainstorm and study.  Right now I'm typing this up in a farm house at an
equestrian eventing farm out in Kennewick, WA.

   I want to be as verbose as possible about my design choices and thought
process in this assignment, because for me it's the best way to get quality
feedback.  Even if I don't get this job, I still get 4-5 hours at Apptio HQ to
pick your guys' brains to find out what you all would've done, and I want to make
every second of that opportunity count.  

    Anyway, I decided to do problem A.  Problem B is pretty similar to exercise
4.13 in "The Go Programming Language", and I spent some time a few weeks ago
digesting chapter 4 and working on 4.10 & 4.11.  Those exercises were about
interacting with Go JSON library and the github API, so doing something similar
for this coding assignment would feel a bit like cheating.  Problem A has a
bunch of stuff that I haven't worked on in Go yet, and the question forces me
to think like I was part of a team.  I spent hours brainstorming and planning
solutions to different scenarios, and I'm really excited to see what you guys
think about some of my ideas.

Thanks for your time!

-Brian Bright


#########
PROBLEM A
#########

	The app devs have created a web app that generates a log file of the format:

	<datetime logtime, string "message">

	Your team has deployed the app in production, where the devs don't have access
	to the log file on the servers local file system due to a security boundary.

	The app devs want access to the contents of the log.

	Write automation/program that will allow the devs to view the logs
	without manual intervention.


#######################
DESIGN & IMPLEMENTATION
#######################

    My plan: install an HTTP server on the main app host machine, listening on
a port that is inaccessible from outside the firewall.  Pretty simple,
just add a rule in iptables/ACL preventing WAN access on that port, and test it
later from the outside with noisy nmap scans.  Thus the app would only respond
to GET requests originating from inside the organization, and could reply with
JSON, plain text, or HTML.

   However, that HTTP design completely depends on the devs accessing the
server from a private IP address.  If the main app were in a container out in
the cloud, this would be a little trickier.  Port forwarding/NAT could be done
for requests coming from a specific range of IP addresses, but that stuff can
be spoofed easily.  Another option would be to write a client side proxy log
server that uses an SSH tunnel to connect with the log server on the main
machine.  A client side proxy would solve many other issues as well, by sort of
trunking requests from devs so that the main machine log server would only need
to track one TCP connection.

  From what I've seen, kubectl has the same issue - insider access for devs,
but none for the clientele.  Thus, if the main app in the problem is
cloud-based, I'd have to either
  a) install keys for each dev
  or
  b) write a client-side proxy server that operates in the organization's
     internal network, and install a key for that server in the worker nodes
     for the main app.
  or
  c) have some kind of ticket authentication/authorization service like kerberos
     configured for the log server.

    Writing a client-side proxy is definitely a goal of mine, if I get that
far. Log entries will never be changed - so if the client requests a search of
entries in a range that the proxy has cached, there's no need to ask the main
app server for a log update.  In the client-side program, caching would reduce
network load.  In the server side program, caching would reduce system calls &
disk I/O requests, leaving more resources available for the main app.

Further ideas/issues:
     - have the server take a conf file so the admin doesn't need a long string of flags
        for each startup of the program
	   *** did this, but see the configuration section for more details.  it's
	       crazy how dependent the app is on deployment - basically, need to come up with
	       deployment before you start coding configurations.
	       
     - big read requests of the log file could slow down the main app
         Possible solutions:
           - add a limit to how much a user can access at one time
	   - only run the search when the CPU isn't under heavy load
	   - before any requests, cache as much of the log file into memory as possible
	   - cache results (both client side and server side)
	   - depending on the OS, change the 'nice' level of the log server process
         	   *** turns out all this cgroup stuff can easily be implemented by Dockerfile
	
     - the server side app will need read access on the log file and any
        archived log files.
     - network traffic of sending log search results could impact performance for main app
     - upgrades to the server side program API shouldn't break clients that assume
         an older API
     - if using a proxy, could batch requests & send as one to the log server
     - filtering the log file kinda reminds me of MapReduce; curious if there's anything I can
        do with that. 
     - big conf.json files are boring to write, could have an interactive CLI program
       to ask questions and create a conf.json file.  Hilariously, that's similar to
       problem B.  See why I chose A?  Much more interesting. 
       

BIG ISSUE: I don't know how the <datetime logtime> is formatted.  Go provides a
   time.Parse function that turns strings into Time values, but it needs either
   an example string or one of the constants in the time library.  I thought
   about having a control flow switch if no example string is provided, meaning
   it would keep the datetime entry as a string, but since the other case would
   return a value of Time, I'd have to have two different structs for log
   entries.  In the real world, this wouldn't be an issue for this problem
   because the format should be known. It's a frustrating thing to think about,
   but nonetheless an interesting one.
   I can't really have a default example string, because that would break
   anything in the wild pretty badly if it didn't use that format.  However, my
   current solution splits the log entries up by "," and assumes that the first
   comma is the first byte after the datetime string.  If the datetime format
   used in the main app has commas in it, I'm absolutely hosed.

Damned if you do, damned if you don't.  I think I'll keep it the way it is for
now.

    Obviously, there are tons of issues I could work on.  However, the
assignment isn't supposed to be a month long affair, and writing something
overly complex might be frowned upon. My goal is to see what I can write in 3-4
hours, and add stuff on from there.  Overall I expect my time spent on this
assignment to be fairly high, because I'm not an expert in Go and I want to
sharpen my skills.


###################
Language choice: Go
###################

The server will be written in Go, for a few reasons.
   1. Since Go compiles statically linked binaries, library dependencies
      	will not be an issue, making both programs easier to install.
   2. Go tools make it easy to compile into many different architectures/OS's 
   3. Go has really nice built-in concurrency that the server side program
        can take advantage of if multiple requests come in simultaneously
   4. The proxy server I wrote in C is pretty similar - copy/pasting would be no fun.
   5. Go is a lot more secure than anything I'd write in C


############
ALTERNATIVES
############

 - log.Fatal(http.ListenAndServe(":8080", http.FileServer(http.Dir("/usr/share/doc"))))

    found this on the golang.org/pkg/net/http page.  If the log file were used
        in that http.Dir call, the solution go program would be ~10 lines of
        code.
	-problem: pretty brittle.  If the log file gets large or
        archived, more code would need to be added to accomodate changes.
        However, lazyness is a virtue.  It's the simplest & quickest solution
        to the problem we're trying to solve.
	       
- Give every dev root access on the app server
    - obviously stupid.  Gotta limit the circle of trust; there's no telling what could happen
      when all devs have root. 

- Write a caching client proxy server that forwards dev requests.  If all devs go through
   the proxy, that cache is gonna be magnificent - it will reduce the CPU time and network
   bandwidth taken by the log server
   - problem:
	It's a little beyond the scope of the problem we're trying to solve. Without more
	information, it's impossible to know if caching log files external to the server is
	a good thing or a bad thing.
      
- Add rsyslog to the main app host machine & copy the log files to an external database
     -problem: 
     	       Not enough information is known about the situation.  Rsyslog is more
	       resource intensive than my small log server implementation - maybe smaller
	       is better. 

- Add Kerberos to the network & check tickets whenever log files are requested.
   Adding authentication/authorization to the log server would allow for a much
   more dynamic API.  Server admin specific commands could be added,
   e.g. change any set configurations.
      -problems:
        - increases complexity of the system as a whole
        - Even though main app customers wouldn't need to go through kerb, they might still
	  see slowdowns.  A dev might write a script that grabs log files every 10 seconds,
	  and overhead of decrypting/encrypting those messages by the server app on the host machine
	  could totally bog down the system.
	- This is beyond the scope of the main problem - getting the devs access to the log files.
	  Would definitely need a team of devs & testers to implement something like this.
    
- Chmod the log files to a group on the host OS, give that group read privileges,
   give devs an account on the server they can SSH into, and add that account to the group. a cron job
   could be added to the host to add log archive files to that group with read privileges.
     -problem:
         - assumes the security policy is ok with devs having ssh access to the machine.
	 - would be hard to track which dev is using the account and at what time

- Add in a sandbox program to let devs pass in their own functions to filter the log
     -problem:
	complexity.  Just thought it was a cool idea  
        

###############
DEPLOYMENT PLAN
###############

Deploying such a server would depend heavily on the infrastructure design.  In
the most basic scenario, an admin could write a script that run like so:

a) scp the log server binary that is arch/OS specific to the host machine into
   the same namespace as the log files
    - and don't forget to scp the conf.json file as well
    
b) ssh to the host machine

c) chmod/chown the binary
    - make sure the server binary isn't root
    - create a group that only the binary & the log files are part of
    - give the group read only privileges to the log files

d) write either a cron job or a systemd service that adds any new or archived
   log files to the group

e) have cron or systemd start/restart the server when needed

f) On whatever machine/router connects to the WAN & controls NAT/port
    forwarding, add a rule to the ACL preventing access from WAN IP's to the
    port used by the server.
    
a) through e) would need to be done for each machine the app is run on, so the
script would have to take a list of IP's (either from the CLI or a conf file).

I'm still a beginner with Docker and everything else in the cloud stack, so
coming up with a deployment plan for that kind of infrastructure might take me
a week to learn.  Hopefully I can update this section more before the
interview.

UPDATE: Just got a crash-course in Docker & Dockerfiles.  Verrry
interesting stuff.  Looks like I won't have to use config files nor
bash scripts to deploy this program on multiple hosts; the allure of
Docker is becoming clear.  Wow, you can add CPU and memory constraints
super easily... which is a big part of what I wanted to do to the
logserver process to keep the main app clients happy.

###########################
MONITORING/REPORTING/ALERTS
###########################

Things to log:
  a) config file (parameters, entire file itself, or just vars inside if too long)
     used at runtime
  b) request IP origins for each interaction, possibly what they request as well

Alerts:
  - if one IP address is making a ton of requests - having the main app be DDOSed
     by a log server wouldn't be fun to explain to clients.

Monitoring:
  - use REST heartbeats from an external source to make sure the server is still running.
    I saw a fairly easy way to do this in a lynda video last night, might implement
    it if I have time.
  

Interestingly, Go makes it easy to have a log file on a different machine -
output could be written to a socket, since log.SetOutput takes an io.Writer
interface as the parameter.  Easy remote logging!

#############
CONFIGURATION
#############

This accidentally turned into a blog post, so I moved my thoughts on
configuration down to the bottom of this document.

There's lots of parameters to set on the logserver.  One area of parameters
would be cgroup settings like maximum memory allowed, max cpu resources, stuff
like that, which would be set in the Dockerfile.

Then there's program specific configuration, such as
 a) port to bind on
 b) maximum cache size
 c) location of log file for the log server
 d) config file location
 e) could enable or disable specific RPC methods
 f) where the location of the main app's log file is
 g) what the time ordering is in <datetime logtime> - dd/mm/yy, mm/dd/yy, etc.
    - did some more digging: the time pkg in go has a parser for strings,
      but it needs either the RFC corresponding to the layout, or an example
      of the form to be used by the parser in time.Parse()

      Since there's no authentication/authorization of the incoming requests, every
configuration value must be set on execution.  If anyone could change log
server settings via RPC without a/a, the settings would be pretty meaningless.

      Something that Dockerfile and cgroups don't do is throttle network
traffic for a service.  The most upvoted question in the Kubernetes Dev Team
AMA on reddit (4/10/18) is a question about adding network limits to cgroup
settings, and the answers from the devs were quite illuminating on the subject.
Basically they say it's hard to limit bandwidth because you'd need a controller
on each end, sender and receiver, since network bandwidth isn't a property of a
pod - it's a property of the path from one endpoint to another. Their answers
on that question and other questions led me to research Envoy, which is a
really interesting program.

      From what I've gathered thus far, Envoy gets installed on every pod and
all traffic to and from that pod is proxied through Envoy.  Thus each instance
of Envoy would be able to talk to the receiving service's Envoy instance, and
determine how fast the data should be sent from one side to another.  Honestly
I'm probably wrong in some of that but it sounds... right, I suppose.  I'm
gonna do a lot more research on Envoy.  I'm really curious how they decide to
drop packets if the rate is limited and the queue for packets gets to be too
long, because I could see a situation where there's a long queue when limits
are enforced yet network traffic is low.  That's probably an easy fix in the
protocol though.  All the stuff I've heard about Envoy so far reminds me of
MPLS and some of the other L2 protocols I learned about back when I was
studying for the CCNA.  I'm curious if there's anything at the lower level that
could be adapted for this issue. 


#######
TESTING
#######

Use noisy nmap scans from outside the organization to make sure the port is
inaccessible from the WAN.
   
Obviously unit testing & coverage checks of the code before deployment.

test what happens when a request to the log server times out

Could use a fuzzer to see if any of the parsing Go does in the libraries
is broken

After passing all the small unit tests, we should test what happens when the
log server and the main app run together on the same machine.  The main app is
live and presumably, we shouldn't take it offline.  Therefore, we need to run a
copy of the main app on an internal server alongside the log server.  We need
to see what happens when both the main app and the log server are operating
under heavy request volume, so tests should be written to simulate both
actions.


#########
RESOURCES
#########

I wasn't sure if I was supposed to list only borrowed code, or list every
source that gave me input towards a decision, so I went with the latter.
Figure it's best - especially in this section - to go a little over-the-top.

### Idea sources

My HTTP proxy server solves a similar problem.
   a) take requests from clients over a socket
   b) read in and parse the requests
   c) access the requested resources
   d) write the results back to the client socket
   e) cache results for quicker access next time

"The Go Programming Language" - entire book really

MIT 6.824 Lecture 7 - Guest lecture on Go by Russ Cox of Google/Go to get some 
https://www.youtube.com/watch?v=grrJxx9PKqE

man pages

MIT 6.858 Security - Lecture 4: Privelege Seperation in Web Apps
    https://www.youtube.com/watch?v=XnBJc3-N2BU

Googled 'conf files in golang' trying to find out if there was a conf
parsing library for Go, but most comments I found said to use JSON.

### Code sources

golang.org/pkg/* - used a lot of example code from different package docs. 

From lynda.com's "Advanced Go microservice implementation" - Leander Reimer -
   might borrow his "fun port() string { port := os.Getenv("PORT")
    return ":" + port }
   for reading env vars.

Was reading the Kubernetes team AMA on reddit.com/r/kubernetes 4-10-2018,
   and they mentioned how excited the were about something called 'Envoy'.
   Watched some vids from Matt Klein of Lyft explaining it, got a few ideas.


   


######
ASIDES
######

Hopefully these asides don't make me look foolish or anything.  Just a lot of
thoughts I had while working on this project.

###############
Security aside
###############

    The devs put this app in production - so anyone on the web can access it,
right?  And the log file is being stored on the SAME MACHINE as the app, right?
So, if an attacker were able to compromise the app, even if the attacker
couldn't get root the log file could be modified (to varying degrees, depding
on if the app had read/write privileges or just append only). Worst case
scenario, would be if the attacker were to gain root on the machine, nothing in
the log file would be trustworthy.  The situation makes me think of a race
condition: until the log file is secure, it can't be determined if any
information in it is accurate.

    Thus my first step would be to cross my fingers & pray that the app hasn't been
compromised yet.  If I could, I'd write a script to install & configure rsyslog
on every deployed server to get those log files stored remotely ASAP.  If
rsyslog wasn't an option, I'd write a small app that would monitor the log file
for changes and write those changes to a remote destination, making sure that
the receiving app at the destination could only append log files.  There's
still a vulnerability that the receiving log app would be vulnerable to
malformed RPC from a compromised main app, but it's still a lot more secure of
an implementation.

    Remote storage is beneficial in other ways, beyond security. If the main app
crashes or the host is DDOS'ed, you'd want the log files stored remotely so
that the devs could see what led up to the outage.

HOWEVER, this stuff is obviously beyond the scope of the assignment, but it was
really bugging me so I felt like I should write it down.


###################
Configuration Aside
###################

Ooooh boy.  So there's a feeling I've been getting a lot lately - that feeling
when you uncover a huge topic in the tech world that you never knew about and
you're flabbergasted that you missed it until now.  It's 4/10/2018 and I just
got that feeling for configurations in deploying cloud native applications.

The first configuration steps I took for this problem were to use Go's "flag"
package.  It's so much easier compared to reading argv[] in C that I probably
overdid it.  I had a list of like 5-10 flags that the CLI passed when it ran
the logserver, so I decided to look up config files in Go.  Folks on
StackOverflow reccomended using JSON in a config.json file, so I went with
that.  Seems a lot easier than having to keep track of all the flag settings
passed in at exec time - especially when it's a deployed app and you want to
automate any restarts.

AND THEN I watched a lynda.com video last night from a guy with 20 years
experience and is a self-professed "total Cloud Native nerd" - Leander Reimer.
In his Go code, he pulls configurable variables from the environment!  It kinda
blew my mind, cause it's such a pain in the butt in C that I never really
considered it.

[paraphrasing from Leander Reimer] - "implementing a helper function to extract
the port from an environment variable is an example of good practice in cloud
native application development - you want things configurable via environment
variables"

So that changes things a bit.  Once I saw how you could use Dockerfiles to set
environment variables in the parent process, it really made a lot of sense to
me.  I do have some concerns still, however - if that parent process is
starting multiple children, wouldn't the names of environment variables have to
be unique to not overwrite each other?  You'd have to enforce a naming policy -
easy in theory, hard in practice, like getting people to use hard passwords.

I did some more digging, and the top response to a discussion on StackOverflow
was very illuminating.

https://stackoverflow.com/questions/7443366/argument-passing-strategy-environment-variables-vs-command-line?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa

Definitely read the response from Matt Fenwick.  It's 7 years old, and before
the cloud app takeover, but I bet a lot of it holds true.  Overall he argues
heavily against env vars, and gives examples of the name conflict bugs i was
worried about. He's a proponent of using all 3 methods - flags, env vars, and
config files - so I guess I'll just have to learn the situations in which to
use the proper tool for the job. Interestingly, he distinguishes between real
config files and the config.json file that I've been using.  He calls it the
`command-line parameter-file` approach, because it doesn't store user-set
configuration data.  I suppose a good method for my logserver would be to check
passed-in arguments against a saved config file, and update the config file if
the values differ, then set program variables from the config file. Really
curious what you Apptio guys do - probably something akin to the right tool for
the right job.



https://www.mirantis.com/blog/how-do-you-build-12-factor-apps-using-kubernetes/

This blog post is a great read for the config debate.  It was written 4/17/2017
and focuses specifically on cloud native apps with kubernetes. Looks like the
debate still rages on.












##############
Old stuff, might not use anymore
##############

I've learned a lot over the past few days that answered a lot of questions I
had when I first wrote this README.  I'm keeping it below for historical reasons. 



################
INITIAL THOUGHTS
################

    There are a bunch of ways to tackle this issue.  A few are ruled out due to
the need for automation, and a few others because of manual intervention, a few
more due to their complexity, and a few due to security.

The problem as stated was pretty vague, so I made a few assumptions.

   a) a server admin has root access to every main app host machine
   b) giving all the devs root access to the server isn't a viable solution
   c) Archived log files or the host machine running out of disk space is
       beyond the scope of this assignment
   d) each log entry ends in a new line
   
Please let me know if any of my assumptions are incorrect!  



